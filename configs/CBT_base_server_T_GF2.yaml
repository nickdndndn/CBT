# data pipeline settings
data_pipeline:
  train:
    dataset: GaoFen2
    path: "data/pansharpenning_dataset/GF2/train/train_gf2.h5"
    mslr_img_size: [16, 16]
    pan_img_size: [64, 64]

    preprocessing:
      shuffle: True
      cropping_on_the_fly: False
      RandomHorizontalFlip:
        enable: True
        prob: 0.3
      RandomVerticalFlip:
        enable: True
        prob: 0.3
      RandomRotation:
        enable: False
        degrees: 90

  validation:
    dataset: GaoFen2
    path: "data/pansharpenning_dataset/GF2/val/valid_gf2.h5"
    data_pipeline:
    mslr_img_size: [16, 16]
    pan_img_size: [64, 64]
    preprocessing:
      shuffle: True
      cropping_on_the_fly: False
    val_steps: 100 #leave empty to evaluate the entire validation set

  test:
    dataset: GaoFen2
    path: "data/pansharpenning_dataset/GF2/test/test_gf2_multiExm1.h5"
    mslr_img_size: [64, 64]
    pan_img_size: [256, 256]
    preprocessing:
      shuffle: False
      cropping_on_the_fly: False

machine_settings:
  num_gpu: 2

# general settings
general_settings:
  name: "CBT_base_tiny_GF2"
  model_type: CBT
  continue_from_checkpoint: False
  checkpoint_name: CBT_base_tiny_GF2_2023_08_03-07_13_43.pth.tar
  report_interval: 50
  save_interval: 5000
  evaluation_interval: [50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000]
  test_intervals: [50000, 100000, 150000, 200000, 250000, 300000, 350000, 400000, 450000, 500000]

task:
  upscale: 1
  mslr_to_pan_scale: 4

# network structures
network:
  patch_size: 1
  in_chans: 4
  embed_dim: 30
  depths: [2]
  num_heads: [2]
  window_size: 8
  compress_ratio: 24
  squeeze_factor: 24
  conv_scale: 0.01
  overlap_ratio: 0.5
  mlp_ratio: 2
  qkv_bias: True
  qk_scale: #empty == None
  drop_rate: 0.
  attn_drop_rate: 0.
  drop_path_rate: 0.1
  norm_layer: nn.LayerNorm
  ape: False
  patch_norm: True
  img_range: 1.
  upsampler: "pixelshuffle"
  resi_connection: "1conv"
  hab_wav: True
  scbab_wav: True
  ocab_wav: True
  ocbab_wav: True

training_settings:
  steps: 600000
  batch_size: 8
  optimizer:
    type: Adam
    learning_rate: !!float 4e-4
    betas: [0.9, 0.99]
  scheduler:
    type: StepLR
    lr_decay_intervals: [150000, 250000, 400000, 500000, 550000, 575000]
    gamma: 0.5
  loss:
    type: L1Loss
